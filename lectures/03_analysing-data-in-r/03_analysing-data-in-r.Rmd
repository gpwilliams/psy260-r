---
title: "Summarising and Plotting Data in R"
subtitle: "Analysing Data in R"
author: "Glenn Williams"
institute: "University of Sunderland"
date: "2021-10-27 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<style type="text/css">
.left-code {
  width: 58%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 40%;
  float: right;
  padding-left: 1%;
}
.centre-plot[
  margin: auto;
  width: 25%;
]
</style>

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
set.seed(100)
library(tidyverse)
library(here)

options(tibble.print_min = 4)
knitr::opts_chunk$set(dpi = 600, retina = 1, warning = FALSE, out.width="75%")
```

```{r functions, include = FALSE}
colorise <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```

# Some Background

Data analysis is surprisingly one of the easiest parts of working with R.

- Once your data is in the correct (long) format, analysis using any test is highly consistent.

- We rely on a formula interface like this:

```{r formula-simple, eval = FALSE}
DV ~ IV
```
    
- Our dependent variable/predicted variable goes to the left of the ~ (tilde), while our independent variables or predictors go to the right.
    
- After this we specify our data:

```{r formula-extended, eval = FALSE}
DV ~ IV, data
```

We then apply a function to our formula which is the name of our test. There's some minor options we can choose within tests, but that's pretty much it!

---
# Correlations
## The Data

Let's check out the **starwars** data set again. We'll use this for our tests.

```{r load-data}
starwars <- starwars %>% filter(mass < 500)
```

We will use the height and mass columns, looking at whether mass is associated with height.

```{r glimpse-starwars, echo = FALSE}
glimpse(starwars)
```

---
# Correlation

- Here, we aren't predicting any one variable from the other, so both variables go to the right of the tilde. 

- We add multiple variables with a `+`.

- We choose the type of correlation we want (e.g. Pearson, Spearman) with the method.

```{r correlation}
cor.test(~ height + mass, starwars, method = "pearson")
```

---
# Tests of Difference

- We'll use some different data here on out. 

- Let's assume this data looks at giving people a placebo or drug, and tests the effect of that drug at two different time points. 

- We care about improvements in reaction times.

```{r read-cleaned-data, message = FALSE}
mixed_data <- read_csv(here("data", "mixed_factorial.csv"))
head(mixed_data)
```

---
# t-tests
## One-sample t-test

- We have only one variable here, so we don't even need a formula. 

- We compare the mean of this variable against a specified baseline mean (here 400).

```{r one-sample-t-test}
t.test(mixed_data$rt, mu = 400)
```
---
# t-tests
## Independent-samples t-test

- Do reaction times vary depending on the drug given to participants?

- We test reaction times predicted by drug, with a regular t-test where variances are assumed to be equal (`var.equal = TRUE`).

```{r independent-samples-t-test}
t.test(rt ~ drug, mixed_data, var.equal = TRUE)
```

---
# t-tests
## Paired t-test

- Do reaction times vary over time (i.e. practice)?

- We test reaction times predicted by the time of testing. This is a paired test (`paired = TRUE`) and a regular t-test where variances are assumed to be equal (`var.equal = TRUE`).

```{r paired-samples-t-test}
t.test(rt ~ time, mixed_data, paired = TRUE, var.equal = TRUE)
```

---
# One-way ANOVA
### Between-subjects

- What if we had **more than two groups** for the drug condition? We use an ANOVA.

- We simply change the test function to `aov()` (**A**nalysis **O**f **V**ariance)

- We need to summarise the model results here to get a regular ANOVA output.

```{r one-way-anova-between}
summary(aov(rt ~ drug, mixed_data))
```

---
# One-way ANOVA
### Within-subjects

- What if we have more than two groups and a **within-subjects design**?

- We do the same as before, but need to add an **Error term** to the formula. This states that we adjust our errors to account for the fact scores in each group belong to the same participant (i.e. **id** in our data).

```{r one-way-anova-within}
summary(aov(rt ~ time + Error(id), mixed_data))
```

---
# Two-Way ANOVA
## Mixed

```{r two-way-anova-mixed}
summary(aov(rt ~ time * drug + Error(id), mixed_data))
```

---
# Throw Away the Alphabet Soup

All of the statistical tests you know (e.g. *t*-tests, ANOVA, chi-square) are just extensions of the **general linear model**. This is the most important thing you can learn to use in statistics.

Learn the mean and *variance* of some measurement by using an additive combination of other measurements.

- The **geocentric model of applied statistics**: used wisely, can be useful. But we shouldn't read too much into the numbers produced. They're almost certainly wrong because we can't (and shouldn't) model all sources of variance.

- Predict a **linear relationship** between one or more variable(s) and a continuous (e.g. scale) dependent variable.

- Predictor variables can be continuous or categorical.

---
# Linear Regression

Takes the general form:

$$Y = \alpha + \beta X + e$$

- **Outcome** $Y$ = intercept + (slope $\times$ X) + residual error

- **Residuals** $e$ = distance of observed values from predicted values

- *Note*: We do not fit a perfect model, hence the error term. This is a good thing, otherwise we are probably **overfitting** to our data; relying too much on our observed sample to draw infferences.

---
# Linear Regression

Takes the general form: 

$$Y = \alpha + \beta X + e$$

- The **intercept**, $\alpha$, is usually the point on the y-axis at the lowest value of X (usually 0).

- The **slope**, $\beta$, corresponds to how much Y increases by for every increment in X.

- The **error**, $e$, corresponds to a constant by which to add to our estimates accounting for additional variation from other sources that we do not model.

---
# Linear Regression

Fit the model predicting height from weight from the starwars data.

$$Y = \alpha + \beta X + e$$
$$height = intercept + slope \times mass + error$$

```{r linear-model}
starlm <- lm(height ~ mass, starwars)
summary(starlm)
```

---
# Comparing tests we know...
## Correlation

```{r correlation-comparison}
broom::tidy(cor.test(~ height + mass, starwars, method = "pearson"))
```

```{r lm-correlation-comparison}
broom::tidy(lm(height ~ mass, starwars, method = "pearson"))
```

- *t* statistics match exactly.

---
# Comparing tests we know...
## t-tests

```{r t-test-comparison}
broom::tidy(t.test(rt ~ drug, mixed_data, var.equal = TRUE))
```
```{r lm-t-test-comparison}
broom::tidy(summary(lm(rt ~ drug, mixed_data)))
```
- *t* statistics match exactly.

---
# Comparing tests we know...
## ANOVA

```{r anova-comparison}
summary(aov(rt ~ drug, mixed_data))
```

```{r lm-anova-comparison}
broom::tidy(lm(rt ~ drug, mixed_data))
```

- *t* to *F* is just *t* squared. So, 3.732 squared = 13.93...

---
# Bye!

```{r bye-gif, echo=FALSE, out.width = 450, fig.align="center"}
knitr::include_graphics("https://media.giphy.com/media/SbtWGvMSmJIaV8faS8/giphy.gif")
```

*Effect sizes are easily handled by the {effectsize} package. Super-easy ANOVAs are done using the {afex} package*.