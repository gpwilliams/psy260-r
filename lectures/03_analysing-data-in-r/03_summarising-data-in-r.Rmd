---
title: "Summarising and Plotting Data in R"
subtitle: "Summarising Data"
author: "Glenn Williams"
institute: "University of Sunderland"
date: "2021-10-27 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<style type="text/css">
.left-code {
  width: 58%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 40%;
  float: right;
  padding-left: 1%;
}
.centre-plot[
  margin: auto;
  width: 25%;
]
</style>

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
set.seed(100)
library(tidyverse)
library(here)

options(tibble.print_min = 3)
```

```{r functions, include = FALSE}
colorise <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```

---
# Summarising Data

Individual data points are some **abstraction away from reality**, essentially removing some detail.

- Even with trial-level data, we will **summarise responses** in terms of reaction times or correct responses.

- This removes a lot of the details of the real experience, but allows us to **see patterns**.

- Similarly, we can summarise data from many participants in a number of ways to help communicate a pattern or idea.

We can present summaries of data as a **plot**, as a **table of descriptive statistics**, and make inferences and decisions based on summaries of the data by **modelling our data**.

---
# Descriptive Statistics

We often summarise data in terms of measures of **central tendency** and **dispersion**.

- **Central tendency**: What is the average score? This can be determined in a few ways.

    - **Mean**: used to describe data that are normally distributed. Can be misleading when outliers are present.

    - **Median**: used to describe data that are skewed in some way away from a normal distribution. Suppresses the impact of outliers.
    
.pull-left[
```{r central-tendency-mean}
noskew <- c(10, 20, 15, 20, 22)
mean(noskew)

skew <- c(10, 20, 15, 20, 2000)
mean(skew)
```
]

.pull-right[
```{r central-tendency-median}
median(noskew)

median(skew)
```

]

---
# Descriptive Statistics

- **Dispersion**: The spread of scores (e.g. for individuals) around an average.

    - **Standard Deviation**: the spread of the data from the sample mean. With a normal distribution approximately 1, 2, and 3 *SD*s capture approximately 68, 95, and 99.7% of the data.

    - **Interquartile Range**: Rank orders data into four equal parts; the first region – or quartile – (i.e. 25%), the median (i.e. 50%), and the middle part of the third quartile (i.e. 75%). Used for non-normal data.
    
```{r dispersion}
# low variability in scores = low SD
low_dispersion <- c(10, 12, 8, 9, 11, 10, 10.5, 9, 11.5)
sd(low_dispersion)

# lots of variability in scores = high SD
high_dispersion <- c(10, 1000, 89, -400, 90, 880, 0)
sd(high_dispersion)
```

---
# Why Make these in R?

- **Scales up easily to new data**: If you write this code to analyse data from 10 participants, you can instantly rerun it for millions.

- **Easier to spot mistakes** when you're writing the recipe.

- **Easier to fix mistakes** with a minor modification instead of repeating every step of the analysis by hand.

- **Repeatability**: If you do a task once in R, you can copy and paste code for a new study. This saves a lot of time in the long run.

- **Reproducibility**: Allows others (and future you) to check work and inspect methods at every step. You'll make science more reliable!

---
# Summarise

Let's take a look at the inbuilt `starwars` data set in the `tidyverse` pacakage. There's a lot of data here! How might we summarise this?

```{r starwars}
data(starwars) # load the data from within the package
glimpse(starwars) # view it
```

---
# Summarise

`summarise()` collapses across all observations in your data set to produce a single row of data.

- `summarise()` takes two main arguments: 

    - 1: what is the **data** set you want to summarise?
    - 2: additional information specifying **how you want to summarise it**.
    
Let's summarise the mean height of the Star Wars characters:

```{r summarise-height}
summarise(starwars, mean_height = mean(height))
```

Oops, we got NA! Why? R by default produces NA for any summary of data containing NA values. (You can't average something that isn't there.)

---
# Summarise

What should we do? We can make a new data set without NAs, or tell `mean()` to ignore them.

```{r summarise-height-na}
summarise(starwars, mean_height = mean(height, na.rm = TRUE))
```

We set the argument **na.rm** in `mean()` to **TRUE**, meaning "Should R remove NAs? TRUE (yes)".

---
# Many Summaries

- What if we want to get **different types of summaries**? Imagine we want the count, mean, and standard deviation of height?

- We also can tell R to generate these summaries on the data by first **dropping any observations with missing values (NA)** in height.

```{r summarise-n-mean-sd}
summarise(
  drop_na(starwars, height), 
  n = n(),
  sd = sd(height),
  mean_height = mean(height)
)
```

- How might we interpret this? We have 81 measured heights of characters. The mean height is 174.36cm, while the standard deviation is 34.77. 

- **What's the SD mean**? With normally distributed data 95% of values fall within $\pm$ 1.96 SD of the mean!

---
# Grouping

- As we can see, the data set contains **very many species** of creates from the Star Wars universe. What if we want summaries of all of them?

- We can add an argument to our data in `summarise()` called `group_by()`.

- Within `group_by()`, we specify our data but also give a column by which to **group the summaries**.

```{r group-by-summarise, message=FALSE}
summarise(
  group_by(starwars, species), 
  mean_height = mean(height, na.rm = TRUE)
)
```

---
# Getting Complicated

We can combine `tidyverse` functions together. We could filter before summarising the data, and even make multiple grouped summaries.

```{r group-by-summarise-complex, message=FALSE}
summarise(
  group_by(filter(starwars, mass < 400), species), 
  mean_height = mean(height, na.rm = TRUE),
  sd_height = sd(height, na.rm = TRUE),
  n = n()
)
```

---
# Interim Recap

We've learned...

- Why we summarise data and recapped on basic **descriptive statistics**.

- **Why we might use R** to summarise our data.

- How to pass **optional arguments** to functions (e.g. na.rm in `mean()` and s`d()`).

- How to make **separate summaries per group** using `group_by()`.

- How to combine tidyverse functions together to get the summaries we need.

---
class: inverse, center, middle

# Analysing Data

---
# Some Background

Data analysis is surprisingly one of the easiest parts of working with R.

- Once your data is in the correct (long) format, analysis using any test is highly consistent.

- We rely on a formula interface like this:

```{r formula-simple, eval = FALSE}
DV ~ IV
```
    
- Our dependent variable/predicted variable goes to the left of the ~ (tilde), while our independent variables or predictors go to the right.
    
- After this we specify our data:

```{r formula-extended, eval = FALSE}
DV ~ IV, data
```

We then apply a function to our formula which is the name of our test. There's some minor options we can choose within tests, but that's pretty much it!

---
# Correlations
## The Data

Let's check out the **starwars** data set again. We'll use this for our tests.

```{r load-data}
starwars <- starwars %>% filter(mass < 500)
```

We will use the height and mass columns, looking at whether mass is associated with height.

```{r glimpse-starwars, echo = FALSE}
glimpse(starwars)
```

---
# Correlation

- Here, we aren't predicting any one variable from the other, so both variables go to the right of the tilde. 

- We add multiple variables with a `+`.

- We choose the type of correlation we want (e.g. Pearson, Spearman) with the method.

```{r correlation}
cor.test(~ height + mass, starwars, method = "pearson")
```

---
# Tests of Difference

- We'll use some different data here on out. 

- Let's assume this data looks at giving people a placebo or drug, and tests the effect of that drug at two different time points. 

- We care about improvements in reaction times.

```{r read-cleaned-data, message = FALSE}
mixed_data <- read_csv(here("data", "mixed_factorial.csv"))
head(mixed_data)
```

---
# t-tests
## One-sample t-test

- We have only one variable here, so we don't even need a formula. 

- We compare the mean of this variable against a specified baseline mean (here 400).

```{r one-sample-t-test}
t.test(mixed_data$rt, mu = 400)
```
---
# t-tests
## Independent-samples t-test

- Do reaction times vary depending on the drug given to participants?

- We test reaction times predicted by drug, with a regular t-test where variances are assumed to be equal (`var.equal = TRUE`).

```{r independent-samples-t-test}
t.test(rt ~ drug, mixed_data, var.equal = TRUE)
```

---
# t-tests
## Paired t-test

- Do reaction times vary over time (i.e. practice)?

- We test reaction times predicted by the time of testing. This is a paired test (`paired = TRUE`) and a regular t-test where variances are assumed to be equal (`var.equal = TRUE`).

```{r paired-samples-t-test}
t.test(rt ~ time, mixed_data, paired = TRUE, var.equal = TRUE)
```

---
# One-way ANOVA
### Between-subjects

- What if we had **more than two groups** for the drug condition? We use an ANOVA.

- We simply change the test function to `aov()` (**A**nalysis **O**f **V**ariance)

- We need to summarise the model results here to get a regular ANOVA output.

```{r one-way-anova-between}
summary(aov(rt ~ drug, mixed_data))
```

---
# One-way ANOVA
### Within-subjects

- What if we have more than two groups and a **within-subjects design**?

- We do the same as before, but need to add an **Error term** to the formula. This states that we adjust our errors to account for the fact scores in each group belong to the same participant (i.e. **id** in our data).

```{r one-way-anova-within}
summary(aov(rt ~ time + Error(id), mixed_data))
```

---
# Two-Way ANOVA
## Mixed

```{r two-way-anova-mixed}
summary(aov(rt ~ time * drug + Error(id), mixed_data))
```

---
# Throw Away the Alphabet Soup

All of the statistical tests you know (e.g. *t*-tests, ANOVA, chi-square) are just extensions of the **general linear model**. This is the most important thing you can learn to use in statistics.

Learn the mean and *variance* of some measurement by using an additive combination of other measurements.

- The **geocentric model of applied statistics**: used wisely, can be useful. But we shouldn't read too much into the numbers produced. They're almost certainly wrong because we can't (and shouldn't) model all sources of variance.

- Predict a **linear relationship** between one or more variable(s) and a continuous (e.g. scale) dependent variable.

- Predictor variables can be continuous or categorical.

---
# Linear Regression

Takes the general form:

$$Y = \alpha + \beta X + e$$

- **Outcome** $Y$ = intercept + (slope $\times$ X) + residual error

- **Residuals** $e$ = distance of observed values from predicted values

- *Note*: We do not fit a perfect model, hence the error term. This is a good thing, otherwise we are probably **overfitting** to our data; relying too much on our observed sample to draw infferences.

---
# Linear Regression

Takes the general form: 

$$Y = \alpha + \beta X + e$$

- The **intercept**, $\alpha$, is usually the point on the y-axis at the lowest value of X (usually 0).

- The **slope**, $\beta$, corresponds to how much Y increases by for every increment in X.

- The **error**, $e$, corresponds to a constant by which to add to our estimates accounting for additional variation from other sources that we do not model.

---
# Linear Regression

Fit the model predicting height from weight from the starwars data.

$$Y = \alpha + \beta X + e$$
$$height = intercept + slope \times mass + error$$

```{r linear-model}
starlm <- lm(height ~ mass, starwars)
summary(starlm)
```

---
# Comparing tests we know...
## Correlation

```{r correlation-comparison}
broom::tidy(cor.test(~ height + mass, starwars, method = "pearson"))
```

```{r lm-correlation-comparison}
broom::tidy(lm(height ~ mass, starwars, method = "pearson"))
```

- *t* statistics match exactly.

---
# Comparing tests we know...
## t-tests

```{r t-test-comparison}
broom::tidy(t.test(rt ~ drug, mixed_data, var.equal = TRUE))
```
```{r lm-t-test-comparison}
broom::tidy(summary(lm(rt ~ drug, mixed_data)))
```
- *t* statistics match exactly.

---
# Comparing tests we know...
## ANOVA

```{r anova-comparison}
summary(aov(rt ~ drug, mixed_data))
```

```{r lm-anova-comparison}
broom::tidy(lm(rt ~ drug, mixed_data))
```

- *t* to *F* is just *t* squared. So, 3.732 squared = 13.93...

---
# Bye!

```{r bye-gif, echo=FALSE, out.width = 450, fig.align="center"}
knitr::include_graphics("https://media.giphy.com/media/SbtWGvMSmJIaV8faS8/giphy.gif")
```

*Effect sizes are easily handled by the {effectsize} package. Super-easy ANOVAs are done using the {afex} package*.

